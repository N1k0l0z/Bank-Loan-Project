{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-2RnBT3IWT2",
        "outputId": "edce6207-2368-43b0-da95-425fac251533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=f3d221f3283c3f98eabd4c74a7f513ef46a811add3e1593fd911759a2a2482a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAcwZlM6IRds",
        "outputId": "0bfe0e08-83ff-40e5-8257-757ead275085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType, FloatType\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.functions import struct\n",
        "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
        "from pyspark.sql.types import NumericType\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import to_date, month, year\n",
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "MHn_Pz-oIVmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Bank Project\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
        "    .config(\"spark.executor.instances\", \"4\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "zkpRLpxNIces"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = spark.read.csv(\"/content/drive/MyDrive/Bank Project/test\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "mbsEhuXnIedN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Bank Project/columns_.pkl', 'rb') as file:\n",
        "    final_cols = pickle.load(file)"
      ],
      "metadata": {
        "id": "FdSCgP9SIz_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.select(*final_cols)"
      ],
      "metadata": {
        "id": "gye45SbNI0lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_rows(data):\n",
        "  def null_percentage(row):\n",
        "    null_count = sum([1 for val in row if val is None])\n",
        "    total_count = len(row)\n",
        "    return (null_count / total_count) * 100\n",
        "\n",
        "  null_percentage_udf = udf(null_percentage, FloatType())\n",
        "\n",
        "  columns = data.columns\n",
        "  df = data.withColumn(\"null_percentage\", null_percentage_udf(struct(*columns)))\n",
        "\n",
        "  filtered_df = df.filter(col(\"null_percentage\") <= 80)\n",
        "\n",
        "  filtered_df = filtered_df.drop(\"null_percentage\")\n",
        "  return filtered_df"
      ],
      "metadata": {
        "id": "YP8C-JbaIuVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_term(data):\n",
        "    data = data.withColumn('term', regexp_extract(col('term'), r'(\\d+)', 1))\n",
        "    return data"
      ],
      "metadata": {
        "id": "NYot_Bl1I4h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_int_rate(data):\n",
        "    data = data.withColumn('int_rate', regexp_replace(col('int_rate'), '%', ''))\n",
        "    return data"
      ],
      "metadata": {
        "id": "6GKPMX5PI6F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_revol_util(data):\n",
        "    data = data.withColumn('revol_util', regexp_replace(col('int_rate'), '%', ''))\n",
        "    return data"
      ],
      "metadata": {
        "id": "TXRwow7mI7Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_emp_length(data):\n",
        "    data = data.withColumn('emp_length', regexp_replace(col('emp_length'), r'\\+', ''))\n",
        "\n",
        "    data = data.withColumn('emp_length', regexp_extract(col('emp_length'), r'(\\d+)', 1))\n",
        "\n",
        "    data = data.withColumn('emp_length', col('emp_length').cast('int'))\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "q9UGUGl2I8vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_grade(data):\n",
        "    def map_sub_grade(sub_grade):\n",
        "        mapping = {\n",
        "            'A1': 0, 'A2': 1, 'A3': 2, 'A4': 3, 'A5': 4,\n",
        "            'B1': 5, 'B2': 6, 'B3': 7, 'B4': 8, 'B5': 9,\n",
        "            'C1': 10, 'C2': 11, 'C3': 12, 'C4': 13, 'C5': 14,\n",
        "            'D1': 15, 'D2': 16, 'D3': 17, 'D4': 18, 'D5': 19,\n",
        "            'E1': 20, 'E2': 21, 'E3': 22, 'E4': 23, 'E5': 24,\n",
        "            'F1': 25, 'F2': 26, 'F3': 27, 'F4': 28, 'F5': 29,\n",
        "            'G1': 30, 'G2': 31, 'G3': 32, 'G4': 33, 'G5': 34\n",
        "        }\n",
        "        return mapping.get(sub_grade, -1)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('sub_grade', map_sub_grade_udf(col('sub_grade')))\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "Xr1I8XWhUR1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_emp_length(data):\n",
        "    data = data.withColumn('emp_length', regexp_replace(col('emp_length'), r'\\+', ''))\n",
        "\n",
        "    data = data.withColumn('emp_length', regexp_extract(col('emp_length'), r'(\\d+)', 1))\n",
        "\n",
        "    data = data.withColumn('emp_length', col('emp_length').cast('int'))\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "f9Tde8K5I-tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_emp_title(data):\n",
        "  def categorize_profession(title):\n",
        "      if not title:\n",
        "          return 'Unknown'\n",
        "      title = title.lower()\n",
        "      if any(keyword in title for keyword in ['manager', 'director', 'vp', 'executive', 'head', 'chief', 'president', 'supervisor', 'coordinator']):\n",
        "          return 'Management and Executive Roles'\n",
        "      elif any(keyword in title for keyword in ['assistant', 'coordinator', 'clerk', 'office', 'receptionist', 'secretary', 'administrator', 'data entry', 'support', 'specialist', 'scheduler']):\n",
        "          return 'Administrative and Support Roles'\n",
        "      elif any(keyword in title for keyword in ['engineer', 'technician', 'developer', 'it', 'analyst', 'architect', 'programmer', 'consultant', 'tech', 'network', 'systems', 'software', 'hardware']):\n",
        "          return 'Technical and Engineering Roles'\n",
        "      elif any(keyword in title for keyword in ['nurse', 'therapist', 'counselor', 'healthcare', 'clinician', 'practitioner', 'physician', 'medical', 'pharmacist', 'social worker', 'aide', 'caregiver']):\n",
        "          return 'Healthcare and Social Services Roles'\n",
        "      elif any(keyword in title for keyword in ['sales', 'customer service', 'representative', 'associate', 'agent', 'account manager', 'client service', 'business development', 'account executive']):\n",
        "          return 'Sales and Customer Service Roles'\n",
        "      elif any(keyword in title for keyword in ['teacher', 'instructor', 'professor', 'lecturer', 'tutor', 'educator', 'trainer', 'academic advisor']):\n",
        "          return 'Education and Training Roles'\n",
        "      elif any(keyword in title for keyword in ['technician', 'mechanic', 'electrician', 'plumber', 'carpenter', 'welder', 'machinist', 'laborer', 'foreman', 'operator']):\n",
        "          return 'Skilled Trades and Labor'\n",
        "      elif any(keyword in title for keyword in ['designer', 'artist', 'graphic designer', 'creative director', 'art director', 'web designer', 'illustrator', 'photographer', 'stylist']):\n",
        "          return 'Creative and Design Roles'\n",
        "      elif any(keyword in title for keyword in ['accountant', 'auditor', 'financial analyst', 'controller', 'bookkeeper', 'tax preparer', 'finance manager', 'investment analyst']):\n",
        "          return 'Finance and Accounting Roles'\n",
        "      elif any(keyword in title for keyword in ['attorney', 'lawyer', 'paralegal', 'legal assistant', 'compliance officer', 'legal advisor', 'legal secretary']):\n",
        "          return 'Legal and Compliance Roles'\n",
        "      else:\n",
        "          return 'Unknown'\n",
        "\n",
        "  categorize_profession_udf = udf(categorize_profession, StringType())\n",
        "\n",
        "  df_categorized = data.withColumn('emp_title', categorize_profession_udf(col('emp_title')))\n",
        "\n",
        "  indexer = StringIndexer(inputCol='emp_title', outputCol='emp_title_index')\n",
        "  df_indexed = indexer.fit(df_categorized).transform(df_categorized)\n",
        "\n",
        "  encoder = OneHotEncoder(inputCols=['emp_title_index'], outputCols=['emp_title_encoded'])\n",
        "  df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "  df_encoded = df_encoded.drop('emp_title_index')\n",
        "  df_encoded = df_encoded.drop('emp_title')\n",
        "  return df_encoded"
      ],
      "metadata": {
        "id": "BUjnkg4fJBnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_home_ownership(data):\n",
        "    def map_sub_grade(value):\n",
        "        mapping = {\n",
        "            'RENT': 0,\n",
        "            'MORTGAGE': 1,\n",
        "            'OWN': 2,\n",
        "        }\n",
        "        return mapping.get(value, 3)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('home_ownership', map_sub_grade_udf(col('home_ownership')))\n",
        "\n",
        "    one_hot_encoder = OneHotEncoder(inputCols=['home_ownership'], outputCols=['home_ownership_encoded'])\n",
        "    df_encoded = one_hot_encoder.fit(df_encoded).transform(df_encoded)\n",
        "    df_encoded = df_encoded.drop('home_ownership')\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "qRIvOApOUFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_verification_status(data):\n",
        "    def map_sub_grade(value):\n",
        "        mapping = {\n",
        "            'Verified': 0,\n",
        "            'Source Verified': 1,\n",
        "            'Not Verified': 2,\n",
        "        }\n",
        "        return mapping.get(value, 3)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('verification_status', map_sub_grade_udf(col('verification_status')))\n",
        "\n",
        "    one_hot_encoder = OneHotEncoder(inputCols=['verification_status'], outputCols=['verification_status_encoded'], dropLast=False)\n",
        "\n",
        "    df_encoded = one_hot_encoder.fit(df_encoded).transform(df_encoded)\n",
        "\n",
        "    df_encoded = df_encoded.drop('verification_status')\n",
        "\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "F1LZPX9LUFin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_loan_status(data):\n",
        "    def map_sub_grade(value):\n",
        "        mapping = {\n",
        "            'Fully Paid': 0,\n",
        "            'Charged Off': 1,\n",
        "            'Current': 2,\n",
        "            'In Grace Period': 3,\n",
        "            'Late (16-30 days)': 4,\n",
        "            'Late (31-120 days)': 5,\n",
        "        }\n",
        "        return mapping.get(value, 6)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('loan_status', map_sub_grade_udf(col('loan_status')))\n",
        "\n",
        "    one_hot_encoder = OneHotEncoder(inputCols=['loan_status'], outputCols=['loan_status_encoded'])\n",
        "    df_encoded = one_hot_encoder.fit(df_encoded).transform(df_encoded)\n",
        "    df_encoded = df_encoded.drop('loan_status')\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "8BeQqc43UFf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_purpose(data):\n",
        "    def map_sub_grade(value):\n",
        "        mapping = {\n",
        "            'wedding': 0,\n",
        "            'educational': 1,\n",
        "            'other': 2,\n",
        "            'small_business': 3,\n",
        "            'debt_consolidation': 4,\n",
        "            'credit_card': 5,\n",
        "            'moving': 6,\n",
        "            'vacation': 7,\n",
        "            'renewable_energy': 8,\n",
        "            'house': 9,\n",
        "            'car': 10,\n",
        "            'major_purchase': 11,\n",
        "            'medical': 12,\n",
        "            'home_improvement': 13\n",
        "        }\n",
        "        return mapping.get(value, 14)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('purpose', map_sub_grade_udf(col('purpose')))\n",
        "\n",
        "    one_hot_encoder = OneHotEncoder(inputCols=['purpose'], outputCols=['purpose_encoded'], dropLast=False)\n",
        "    df_encoded = one_hot_encoder.fit(df_encoded).transform(df_encoded)\n",
        "    df_encoded = df_encoded.drop('purpose')\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "ZSPgEDcQUFd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_addr_state(data):\n",
        "    def map_sub_grade(value):\n",
        "        mapping = {\n",
        "        'AK': 0,\n",
        "        'AL': 1,\n",
        "        'AR': 2,\n",
        "        'AZ': 3,\n",
        "        'CA': 4,\n",
        "        'CO': 5,\n",
        "        'CT': 6,\n",
        "        'DE': 7,\n",
        "        'FL': 8,\n",
        "        'GA': 9,\n",
        "        'HI': 10,\n",
        "        'IA': 11,\n",
        "        'ID': 12,\n",
        "        'IL': 13,\n",
        "        'IN': 14,\n",
        "        'KS': 15,\n",
        "        'KY': 16,\n",
        "        'LA': 17,\n",
        "        'MA': 18,\n",
        "        'MD': 19,\n",
        "        'ME': 20,\n",
        "        'MI': 21,\n",
        "        'MN': 22,\n",
        "        'MO': 23,\n",
        "        'MS': 24,\n",
        "        'MT': 25,\n",
        "        'NC': 26,\n",
        "        'ND': 27,\n",
        "        'NE': 28,\n",
        "        'NH': 29,\n",
        "        'NJ': 30,\n",
        "        'NM': 31,\n",
        "        'NV': 32,\n",
        "        'NY': 33,\n",
        "        'OH': 34,\n",
        "        'OK': 35,\n",
        "        'OR': 36,\n",
        "        'PA': 37,\n",
        "        'RI': 38,\n",
        "        'SC': 39,\n",
        "        'SD': 40,\n",
        "        'TN': 41,\n",
        "        'TX': 42,\n",
        "        'UT': 43,\n",
        "        'VA': 44,\n",
        "        'VT': 45,\n",
        "        'WA': 46,\n",
        "        'WI': 47,\n",
        "        'WV': 48,\n",
        "        'WY': 49}\n",
        "        return mapping.get(value, 50)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('addr_state', map_sub_grade_udf(col('addr_state')))\n",
        "\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "zsDjxn9IUFa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_initial_list_status(data):\n",
        "    def map_sub_grade(value):\n",
        "        mapping = {\n",
        "            'f': 0,\n",
        "            'w': 1,\n",
        "        }\n",
        "        return mapping.get(value, 2)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_sub_grade, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('initial_list_status', map_sub_grade_udf(col('initial_list_status')))\n",
        "\n",
        "    one_hot_encoder = OneHotEncoder(inputCols=['initial_list_status'], outputCols=['initial_list_status_encoded'], dropLast=False)\n",
        "    df_encoded = one_hot_encoder.fit(df_encoded).transform(df_encoded)\n",
        "    df_encoded = df_encoded.drop('initial_list_status')\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "_7BezqUIUFV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_time_to_numeric(data):\n",
        "  lst = ['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d']\n",
        "  input_date_format = \"MMM-yyyy\"\n",
        "  for i in lst:\n",
        "      data = data.withColumn(\"date\", to_date(col(i), input_date_format))\n",
        "\n",
        "      data = data.withColumn(f\"month_{i}\", month(col(\"date\")))\n",
        "      data = data.withColumn(f\"year_{i}\", year(col(\"date\")))\n",
        "      data = data.drop('date', i)\n",
        "  return data"
      ],
      "metadata": {
        "id": "_36VzTBtJE8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_delinq_2yrs(data):\n",
        "    def map_delinq_2yrs(delinq_2yrs):\n",
        "        mapping = {\n",
        "            0: 0,\n",
        "            1: 1,\n",
        "            2: 2,\n",
        "            3: 3,\n",
        "            4: 3\n",
        "        }\n",
        "        return mapping.get(delinq_2yrs, 4)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_delinq_2yrs, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('delinq_2yrs', map_sub_grade_udf(col('delinq_2yrs')))\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "8Xe3SKO5JGsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_acc_now_delinq(data):\n",
        "    def map_acc_now_delinq(acc_now_delinq):\n",
        "        mapping = {\n",
        "            0: 0\n",
        "        }\n",
        "        return mapping.get(acc_now_delinq, 1)\n",
        "\n",
        "    map_sub_grade_udf = udf(map_acc_now_delinq, IntegerType())\n",
        "\n",
        "    df_encoded = data.withColumn('acc_now_delinq', map_sub_grade_udf(col('acc_now_delinq')))\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "aVN_WhbKJH68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cast_int(data):\n",
        "  for i in data.columns:\n",
        "    if data.select(i).dtypes[0][1] == 'string':\n",
        "        data = data.withColumn(i, col(i).cast('integer'))\n",
        "  return data"
      ],
      "metadata": {
        "id": "vi9_fjstJKGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_nulls_with_medians(data):\n",
        "    columnset = data.columns[:7] + data.columns[8:13] + data.columns[14:28] + data.columns[-10:]\n",
        "    train_parameters = spark.read.csv('/content/drive/MyDrive/Bank Project/train_parameters', header=True, inferSchema=True)\n",
        "    medians = data.columns[:7] + data.columns[8:13] + data.columns[14:28] + data.columns[-10:]\n",
        "    for col_name in columnset:\n",
        "        median_value = train_parameters.filter(col('column_name') == col_name).select('median').collect()[0][0]\n",
        "\n",
        "        data = data.withColumn(col_name, when(col(col_name).isNull(), median_value).otherwise(col(col_name)))\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "Q4YGa8orJW2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing_values(spark_df):\n",
        "    with open('/content/drive/MyDrive/Bank Project/median_replacement.pkl', 'rb') as file:\n",
        "        median_replacement_columns = pickle.load(file)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Bank Project/model_replacement.pkl', 'rb') as file:\n",
        "        model_replacement_columns = pickle.load(file)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Bank Project/train_colls.pkl', 'rb') as file:\n",
        "        train_colls = pickle.load(file)\n",
        "    relevant_columns = train_colls + ['acc_now_delinq', 'delinq_2yrs', 'delinq_amnt'] + model_replacement_columns\n",
        "    spark_df = spark_df.select([col for col in spark_df.columns if col in relevant_columns])\n",
        "\n",
        "    spark_df = spark_df.cache()\n",
        "\n",
        "    median_parameters = spark.read.csv('/content/drive/MyDrive/Bank Project/train_parameters', header=True, inferSchema=True)\n",
        "\n",
        "    median_dict = dict(zip(median_parameters.select('column_name').rdd.flatMap(lambda x: x).collect(),\n",
        "                           median_parameters.select('median').rdd.flatMap(lambda x: x).collect()))\n",
        "    for column in median_replacement_columns:\n",
        "        if column in median_dict:\n",
        "            median_value = median_dict[column]\n",
        "            spark_df = spark_df.fillna(median_value, subset=[column])\n",
        "\n",
        "    for column in model_replacement_columns:\n",
        "        model_path = f'/content/drive/MyDrive/Bank Project/{column}_dt_model'\n",
        "        model = DecisionTreeRegressionModel.load(model_path)\n",
        "\n",
        "        assembler = VectorAssembler(inputCols=train_colls, outputCol='features')\n",
        "        feature_df = assembler.transform(spark_df)\n",
        "\n",
        "        predictions = model.transform(feature_df)\n",
        "\n",
        "        predictions_dict = dict(predictions.select(column, 'prediction').rdd.map(lambda row: (row[column], row['prediction'])).collect())\n",
        "\n",
        "        def replace_missing_value(value):\n",
        "            return predictions_dict.get(value, None)\n",
        "\n",
        "        replace_missing_value_udf = F.udf(replace_missing_value, DoubleType())\n",
        "\n",
        "        spark_df = spark_df.withColumn(column, F.when(F.col(column).isNull(), replace_missing_value_udf(F.col(column))).otherwise(F.col(column)))\n",
        "\n",
        "    spark_df.unpersist()\n",
        "\n",
        "    return spark_df"
      ],
      "metadata": {
        "id": "MZUMN0YmJgLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_data(spark, data, min_max_path):\n",
        "    min_max_spark_df = spark.read.csv(min_max_path, header=True, inferSchema=True)\n",
        "\n",
        "    column_name_col = min_max_spark_df.columns[0]\n",
        "    cols_ = data.columns\n",
        "    new_cols = cols_[:10] + cols_[11:] + [cols_[10]]\n",
        "    data = data.select(new_cols)\n",
        "\n",
        "    for column in data.columns:\n",
        "        if column == 'acc_now_delinq':\n",
        "            continue\n",
        "        if column == 'delinq_2yrs':\n",
        "            continue\n",
        "        min_row = min_max_spark_df.filter(F.col(column_name_col) == column).select('min').first()\n",
        "        max_row = min_max_spark_df.filter(F.col(column_name_col) == column).select('max').first()\n",
        "\n",
        "        if min_row and max_row:\n",
        "            min_val = min_row['min']\n",
        "            max_val = max_row['max']\n",
        "\n",
        "            if min_val == max_val:\n",
        "                print(f\"Column {column} has min_val == max_val == {min_val}. Filling this column with zeros.\")\n",
        "                data = data.withColumn(column, F.lit(0))\n",
        "            else:\n",
        "                data = data.withColumn(column, (F.col(column) - min_val) / (max_val - min_val))\n",
        "        else:\n",
        "            print(f\"Min or Max value for column {column} not found in the min_max_df.\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "c7DroQNoJmJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def round_data(data):\n",
        "    for column, dtype in data.dtypes:\n",
        "        if isinstance(data.schema[column].dataType, NumericType):\n",
        "            data = data.withColumn(column, F.round(F.col(column), 4))\n",
        "    return data"
      ],
      "metadata": {
        "id": "1DT1agt7JrEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_outliers(data):\n",
        "    if data.columns[-1] == 'delinq_amnt':\n",
        "        whisker_path = '/content/drive/MyDrive/Bank Project/whiskers_delinq_amnt.pkl'\n",
        "    elif data.columns[-1] == 'delinq_2yrs':\n",
        "        whisker_path = '/content/drive/MyDrive/Bank Project/whiskers_delinq_2yrs.pkl'\n",
        "    elif data.columns[-1] == 'acc_now_delinq':\n",
        "        whisker_path = '/content/drive/MyDrive/Bank Project/whiskers_acc_now_delinq.pkl'\n",
        "\n",
        "    with open(whisker_path, 'rb') as file:\n",
        "        whiskers = pickle.load(file)\n",
        "\n",
        "    train_parameters = spark.read.csv('/content/drive/MyDrive/Bank Project/train_parameters', header=True, inferSchema=True)\n",
        "\n",
        "    for column in data.columns:\n",
        "        if isinstance(data.schema[column].dataType, VectorUDT):\n",
        "            continue\n",
        "        outliers = True\n",
        "\n",
        "\n",
        "        outliers = False\n",
        "        median_value = train_parameters.filter(col('column_name') == column).select('median').collect()[0][0]\n",
        "        min_value = train_parameters.filter(col('column_name') == column).select('min').collect()[0][0]\n",
        "        max_value = train_parameters.filter(col('column_name') == column).select('max').collect()[0][0]\n",
        "        scaled_median = (median_value - min_value) / (max_value - min_value)\n",
        "\n",
        "        column_index = whiskers['column'].index(column)\n",
        "\n",
        "        lower_whisker = whiskers['lower_whisker'][column_index]\n",
        "        upper_whisker = whiskers['upper_whisker'][column_index]\n",
        "\n",
        "        data = data.withColumn(\n",
        "            column,\n",
        "            when(col(column).isNull(), None)\n",
        "            .when(col(column) <= lower_whisker, scaled_median)\n",
        "            .when(col(column) >= upper_whisker, scaled_median)\n",
        "            .otherwise(col(column))\n",
        "        )\n",
        "    return data"
      ],
      "metadata": {
        "id": "lz7ipSTnEab6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = drop_rows(test)\n",
        "test = process_term(test)\n",
        "test = process_int_rate(test)\n",
        "test = process_revol_util(test)\n",
        "test = process_emp_length(test)\n",
        "test = process_grade(test)\n",
        "test = process_emp_title(test)\n",
        "test = process_home_ownership(test)"
      ],
      "metadata": {
        "id": "Hva_ENVyOtli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = process_verification_status(test)"
      ],
      "metadata": {
        "id": "mfa1ONhKOvdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = process_loan_status(test)\n",
        "test = process_purpose(test)\n",
        "test = process_addr_state(test)\n",
        "test = process_initial_list_status(test)\n",
        "test = from_time_to_numeric(test)\n",
        "test = process_delinq_2yrs(test)\n",
        "test = process_acc_now_delinq(test)\n",
        "test = cast_int(test)"
      ],
      "metadata": {
        "id": "XEo9ZR-FUalO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = replace_nulls_with_medians(test)\n",
        "filled_missing_values = fill_missing_values(test)\n",
        "scaled_data = scale_data(filled_missing_values)\n",
        "rounded_data = round_data(scaled_data)"
      ],
      "metadata": {
        "id": "eSlssBE1VnHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_delinq_amnt = rounded_data.select(*rounded_data.columns[:49] + rounded_data.columns[52:] + [rounded_data.columns[51]])\n",
        "main_acc_now_delinq = rounded_data.select(*rounded_data.columns[:49] + rounded_data.columns[52:] + [rounded_data.columns[49]])\n",
        "main_delinq_2yrs = rounded_data.select(*rounded_data.columns[:49] + rounded_data.columns[52:] + [rounded_data.columns[50]])"
      ],
      "metadata": {
        "id": "fDrRYRcMD4Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_delinq_amnt = handle_outliers(main_delinq_amnt)\n",
        "final_acc_now_delinq = handle_outliers(main_acc_now_delinq)\n",
        "final_delinq_2yrs = handle_outliers(main_delinq_2yrs)"
      ],
      "metadata": {
        "id": "hTAt3MmMEQOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_delinq_amnt.write.parquet('/content/drive/MyDrive/Bank Project/final_delinq_amnt_test', mode='overwrite')\n",
        "final_acc_now_delinq.write.parquet('/content/drive/MyDrive/Bank Project/final_acc_now_delinq_test', mode='overwrite')\n",
        "final_delinq_2yrs.write.parquet('/content/drive/MyDrive/Bank Project/final_delinq_2yrs_test', mode='overwrite')"
      ],
      "metadata": {
        "id": "QafTWTnxK03L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}