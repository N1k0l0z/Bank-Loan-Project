{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGt2c7cFinWO",
        "outputId": "8b8e90b8-e609-4da8-9a9e-29282a1ff6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=6f06ce8d9b0df035ec7f16deabd5efd627f873bd40b117c35f568e675656a9f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eSmHgTyiuXY",
        "outputId": "7633a306-7b4f-4e17-88bd-84a807ed1a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtVODCHVjK_R"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from xgboost.spark import SparkXGBClassifier\n",
        "from xgboost.spark import SparkXGBClassifierModel\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
        "import time\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTwdWTyjSU5"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"XGBoost Bank Loan\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
        "    .config(\"spark.memory.offHeap.size\", \"8g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read train and test data.**"
      ],
      "metadata": {
        "id": "XW0OslUaXA2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdt2rMzsjZ3o"
      },
      "outputs": [],
      "source": [
        "train = spark.read.parquet(\"/content/drive/MyDrive/Bank Project/final_acc_now_delinq_data\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FegH-FpXWc-a"
      },
      "outputs": [],
      "source": [
        "test = spark.read.parquet(\"/content/drive/MyDrive/Bank Project/final_acc_now_delinq_test_\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create vector columns for train and test features.**"
      ],
      "metadata": {
        "id": "1080LvsaXFGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kimLEPf8jtIT"
      },
      "outputs": [],
      "source": [
        "feature_columns_train = train.columns[:-1]\n",
        "assembler = VectorAssembler(inputCols=feature_columns_train, outputCol=\"features\")\n",
        "train = assembler.transform(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbrV6Syz4WKm"
      },
      "outputs": [],
      "source": [
        "feature_columns_test = test.columns[:-1]\n",
        "assembler_test = VectorAssembler(inputCols=feature_columns_test, outputCol=\"features\")\n",
        "test = assembler_test.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuQUCaUlplVZ"
      },
      "source": [
        "# **Build XGBoost Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-0-BU-6kMz4"
      },
      "outputs": [],
      "source": [
        "xgb = SparkXGBClassifier(\n",
        "    features_col=\"features\",\n",
        "    label_col=\"acc_now_delinq\",\n",
        "    prediction_col=\"prediction\",\n",
        "    seed=42,\n",
        "    subsample=0.9,\n",
        "    reg_alpha=0.5,\n",
        "    gamma=0.2,\n",
        "    eval_metric='aucpr',\n",
        "    num_workers=2,\n",
        "    verbosity=1,\n",
        "    max_depth = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters' tuning."
      ],
      "metadata": {
        "id": "ZEPMkRG7XUfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO3Sf6ieMQNR"
      },
      "outputs": [],
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(xgb.n_estimators, [100]) \\\n",
        "    .addGrid(xgb.learning_rate, [0.01, 0.1]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create evaluator."
      ],
      "metadata": {
        "id": "SgSeUSYqXakc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z07s5IofoAMf"
      },
      "outputs": [],
      "source": [
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"acc_now_delinq\",\n",
        "    rawPredictionCol=\"prediction\",\n",
        "    metricName=\"areaUnderPR\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crossvalidation with 3 folds."
      ],
      "metadata": {
        "id": "a7fUj-PxXep9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CrossValidator(estimator=xgb, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2)"
      ],
      "metadata": {
        "id": "dUwovyJyLZ4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = cv.fit(test)"
      ],
      "metadata": {
        "id": "mjMUL8w0Ky5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428ef729-54be-413a-8549-d1cdb398cc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
            "\tbooster params: {'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.2, 'learning_rate': 0.01, 'max_depth': 3, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'subsample': 0.9, 'verbosity': 1, 'seed': 42, 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "INFO:XGBoost-PySpark:Finished xgboost training!\n",
            "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
            "\tbooster params: {'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'subsample': 0.9, 'verbosity': 1, 'seed': 42, 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "INFO:XGBoost-PySpark:Finished xgboost training!\n",
            "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
            "\tbooster params: {'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.2, 'learning_rate': 0.01, 'max_depth': 3, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'subsample': 0.9, 'verbosity': 1, 'seed': 42, 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "INFO:XGBoost-PySpark:Finished xgboost training!\n",
            "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
            "\tbooster params: {'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'subsample': 0.9, 'verbosity': 1, 'seed': 42, 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "INFO:XGBoost-PySpark:Finished xgboost training!\n",
            "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
            "\tbooster params: {'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'subsample': 0.9, 'verbosity': 1, 'seed': 42, 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "INFO:XGBoost-PySpark:Finished xgboost training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation."
      ],
      "metadata": {
        "id": "Z8-lzMaTXNaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = cv_model.transform(test)"
      ],
      "metadata": {
        "id": "eHlc0TvxjRrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrO3pNjWtZ0J"
      },
      "outputs": [],
      "source": [
        "def classification_report(predictions):\n",
        "    # Calculate metrics for class 0\n",
        "    tp_0 = predictions.filter((col(\"acc_now_delinq\") == 0) & (col(\"prediction\") == 0)).count()\n",
        "    fp_0 = predictions.filter((col(\"acc_now_delinq\") == 1) & (col(\"prediction\") == 0)).count()\n",
        "    fn_0 = predictions.filter((col(\"acc_now_delinq\") == 0) & (col(\"prediction\") == 1)).count()\n",
        "    tn_0 = predictions.filter((col(\"acc_now_delinq\") == 1) & (col(\"prediction\") == 1)).count()\n",
        "\n",
        "    precision_0 = tp_0 / (tp_0 + fp_0) if (tp_0 + fp_0) > 0 else 0\n",
        "    recall_0 = tp_0 / (tp_0 + fn_0) if (tp_0 + fn_0) > 0 else 0\n",
        "    f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n",
        "    support_0 = tp_0 + fn_0\n",
        "\n",
        "    # Calculate metrics for class 1\n",
        "    tp_1 = predictions.filter((col(\"acc_now_delinq\") == 1) & (col(\"prediction\") == 1)).count()\n",
        "    fp_1 = predictions.filter((col(\"acc_now_delinq\") == 0) & (col(\"prediction\") == 1)).count()\n",
        "    fn_1 = predictions.filter((col(\"acc_now_delinq\") == 1) & (col(\"prediction\") == 0)).count()\n",
        "    tn_1 = predictions.filter((col(\"acc_now_delinq\") == 0) & (col(\"prediction\") == 0)).count()\n",
        "\n",
        "    precision_1 = tp_1 / (tp_1 + fp_1) if (tp_1 + fp_1) > 0 else 0\n",
        "    recall_1 = tp_1 / (tp_1 + fn_1) if (tp_1 + fn_1) > 0 else 0\n",
        "    f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
        "    support_1 = tp_1 + fn_1\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(f\"Class 0:\")\n",
        "    print(f\"  Precision: {precision_0:.4f}\")\n",
        "    print(f\"  Recall: {recall_0:.4f}\")\n",
        "    print(f\"  F1 Score: {f1_score_0:.4f}\")\n",
        "    print(f\"  Support: {support_0}\")\n",
        "\n",
        "    print(f\"Class 1:\")\n",
        "    print(f\"  Precision: {precision_1:.4f}\")\n",
        "    print(f\"  Recall: {recall_1:.4f}\")\n",
        "    print(f\"  F1 Score: {f1_score_1:.4f}\")\n",
        "    print(f\"  Support: {support_1}\")\n",
        "\n",
        "\n",
        "    roc_auc = evaluator.evaluate(predictions)\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa082m2GGjWO",
        "outputId": "b10441cf-1571-41dc-a4b3-0a3704e386f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "Class 0:\n",
            "  Precision: 0.8300\n",
            "  Recall: 0.9598\n",
            "  F1 Score: 0.8902\n",
            "  Support: 444341\n",
            "Class 1:\n",
            "  Precision: 0.6203\n",
            "  Recall: 0.2505\n",
            "  F1 Score: 0.3569\n",
            "  Support: 116509\n",
            "ROC AUC: 0.4657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Neural Network Model**"
      ],
      "metadata": {
        "id": "VU5tSj5OiWW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from pyspark.ml.functions import vector_to_array"
      ],
      "metadata": {
        "id": "XOMpnswju3pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "train = train.sample(fraction=0.3)\n",
        "X = train.select(\"features\")\n",
        "y = train.select(\"acc_now_delinq\")"
      ],
      "metadata": {
        "id": "taxnHC1vefCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pd = X.toPandas()\n",
        "y_pd = y.toPandas()"
      ],
      "metadata": {
        "id": "FfI1hVm4fJkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_np = np.array(X_pd[\"features\"].tolist())\n",
        "y_np = np.array(y_pd).flatten()"
      ],
      "metadata": {
        "id": "Kv2TXspGf3aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_np.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "adam = Adam(\n",
        "    learning_rate=0.001,\n",
        "    epsilon=1e-07\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "             tf.keras.metrics.AUC(name='auc'),\n",
        "             tf.keras.metrics.Precision(name='precision'),\n",
        "             tf.keras.metrics.Recall(name='recall')]\n",
        ")\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbAAVtRuv_QE",
        "outputId": "3a1c13fe-fc4b-4198-b816-6e20b4c12e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_np, y_np,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_split=0,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4NAoqAi0cDL",
        "outputId": "72f113a9-9159-427c-c3c4-4cbfc74f8bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m21835/21835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3ms/step - accuracy: 0.7719 - auc: 0.8519 - loss: 0.4697 - precision: 0.7555 - recall: 0.8027\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,loss,precision,recall\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m21835/21835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3ms/step - accuracy: 0.8242 - auc: 0.9027 - loss: 0.3869 - precision: 0.8045 - recall: 0.8561\n",
            "Epoch 3/5\n",
            "\u001b[1m21835/21835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 3ms/step - accuracy: 0.8376 - auc: 0.9137 - loss: 0.3627 - precision: 0.8128 - recall: 0.8757\n",
            "Epoch 4/5\n",
            "\u001b[1m21835/21835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3ms/step - accuracy: 0.8463 - auc: 0.9205 - loss: 0.3471 - precision: 0.8195 - recall: 0.8874\n",
            "Epoch 5/5\n",
            "\u001b[1m21835/21835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3ms/step - accuracy: 0.8509 - auc: 0.9245 - loss: 0.3371 - precision: 0.8227 - recall: 0.8945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test.select(\"features\")\n",
        "y_test = test.select(\"acc_now_delinq\")"
      ],
      "metadata": {
        "id": "cj2gfQuqzRvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pd = X_test.toPandas()\n",
        "y_test_pd = y_test.toPandas()"
      ],
      "metadata": {
        "id": "ERwpZ07GzUAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_np = np.array(X_test_pd[\"features\"].tolist())\n",
        "y_test_np = np.array(y_test_pd).flatten()"
      ],
      "metadata": {
        "id": "AGfh3dK0jmxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Model**"
      ],
      "metadata": {
        "id": "cBgOdEZfidDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_np, y_test_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcKNJu6kz61z",
        "outputId": "403331a3-de1e-4ed7-a1e1-71546fe0e589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17527/17527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8002 - auc: 0.7870 - loss: 0.8767 - precision: 0.5140 - recall: 0.6110\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1666195392608643,\n",
              " 0.7640260457992554,\n",
              " 0.7195919156074524,\n",
              " 0.4427328407764435,\n",
              " 0.5254358053207397]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}